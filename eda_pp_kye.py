# -*- coding: utf-8 -*-
"""CSIT946_A2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Lbs32kRiqPcsAAgvITrgRDODpLazD6p1
"""

import pandas as pd
import numpy as np
import seaborn as sns
import zipfile as zp
import os
from matplotlib import pyplot as plt
from sklearn.preprocessing import StandardScaler

# Read the data
df = pd.read_csv('twitter_user_data.csv', encoding='ISO-8859-1')

# Display the data
df.info()
df.head()

"""# Handling Missing Data"""

# Dropping columns with more than 90% missing values
df_cleaned = df.drop(columns=['gender_gold', 'profile_yn_gold', 'tweet_coord'])

# Filling missing values in 'description', 'user_timezone', and 'tweet_location' with a placeholder 'Unknown'
df_cleaned['description'].fillna('Unknown', inplace=True)

df_cleaned['tweet_location'].fillna('Unknown', inplace=True)

# Dropping rows where 'gender' is missing (as it's a small percentage of rows with missing data)
df_cleaned = df_cleaned.dropna(subset=['gender'])

# Drop the 'profile_yn' column since it is not relevant to human/non-human classification
df_cleaned = df_cleaned.drop(columns=['profile_yn'])

# Now that we have handled the missing data, you can proceed with further analysis
df_cleaned.info()  # Display the structure of the cleaned dataset
df_cleaned.head()  # Display the first few rows of the cleaned dataset

"""# Exploratory Data Analysis (EDA)"""

# Distribution of gender
plt.figure(figsize=(8, 6))
sns.countplot(x='gender', data=df_cleaned)
plt.title('Distribution of Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# Distribution of tweet count
plt.figure(figsize=(8, 6))
sns.histplot(df_cleaned['tweet_count'], kde=True, bins=30)
plt.title('Distribution of Tweet Count')
plt.xlabel('Tweet Count')
plt.ylabel('Density')
plt.show()

# Distribution of retweet count
plt.figure(figsize=(8, 6))
sns.histplot(df_cleaned['retweet_count'], kde=True, bins=30)
plt.title('Distribution of Retweet Count')
plt.xlabel('Retweet Count')
plt.ylabel('Density')
plt.show()

# Correlation analysis for numerical features
plt.figure(figsize=(10, 8))
sns.heatmap(df_cleaned[['tweet_count', 'retweet_count', 'fav_number']].corr(), annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Matrix of Numerical Features')
plt.show()

# Extracting date from 'created' and 'tweet_created' for time-based analysis
df_cleaned['profile_created_year'] = pd.to_datetime(df_cleaned['created']).dt.year
df_cleaned['tweet_created_year'] = pd.to_datetime(df_cleaned['tweet_created']).dt.year

# Plotting the distribution of profile creation over the years
plt.figure(figsize=(8, 6))
sns.histplot(df_cleaned['profile_created_year'], kde=False, bins=15)
plt.title('Distribution of Profile Creation Years')
plt.xlabel('Profile Created Year')
plt.ylabel('Count')
plt.show()

# Exploring 'link_color' and 'sidebar_color' features
plt.figure(figsize=(8, 6))
sns.countplot(y='link_color', data=df_cleaned, order=df_cleaned['link_color'].value_counts().iloc[:10].index)
plt.title('Top 10 Most Common Profile Link Colors')
plt.ylabel('Link Color')
plt.xlabel('Count')
plt.show()

plt.figure(figsize=(8, 6))
sns.countplot(y='sidebar_color', data=df_cleaned, order=df_cleaned['sidebar_color'].value_counts().iloc[:10].index)
plt.title('Top 10 Most Common Sidebar Colors')
plt.ylabel('Sidebar Color')
plt.xlabel('Count')
plt.show()

"""# Preprocessing"""

df_cleaned = df_cleaned[df_cleaned['gender'] != 'unknown']

# Scaling numerical features
scaler = StandardScaler()
df_cleaned[['tweet_count', 'retweet_count', 'fav_number']] = scaler.fit_transform(df_cleaned[['tweet_count', 'retweet_count', 'fav_number']])

# change male=>0, female=>1, brand=>2
df_cleaned.loc[df['gender'] == 'male', 'gender'] = 0
df_cleaned.loc[df['gender'] == 'female', 'gender'] = 0
df_cleaned.loc[df['gender'] == 'brand', 'gender'] = 1

# Check the first few rows of the preprocessed data
df_cleaned.head()

import pandas as pd

# First, encode the 'gender' column to numeric values
df_cleaned['gender'] = df_cleaned['gender'].replace({'male': 0, 'female': 1, 'brand': 2})

# Select numerical columns
numerical_columns = ['gender:confidence', 'profile_yn:confidence', 'fav_number', 'retweet_count', 'tweet_count', 'tweet_id']

# Calculate the Pearson correlation with the target variable 'gender'
correlations = df_cleaned[numerical_columns].corrwith(df_cleaned['gender'])

print("Correlations with target (gender):")
print(correlations)

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_selection import chi2
from sklearn.feature_selection import SelectKBest

# List of categorical columns
categorical_columns = ['_unit_state', 'link_color', 'sidebar_color', 'tweet_location', 'user_timezone']  # Add other categorical columns if necessary

# Encode the categorical variables
label_encoder = LabelEncoder()
df_encoded = df_cleaned.copy()
for col in categorical_columns:
    df_encoded[col] = label_encoder.fit_transform(df_encoded[col].astype(str))

# Apply chi-squared test
X = df_encoded[categorical_columns]
y = df_encoded['gender']

chi_scores = chi2(X, y)

print("Chi-square scores:", chi_scores)

import seaborn as sns
import matplotlib.pyplot as plt

# Compute the correlation matrix for numerical columns including 'gender'
corr_matrix = df_cleaned[numerical_columns + ['gender']].corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Heatmap of Numerical Features and Target")
plt.show()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# drop columns that are irrelevant

col = ['gender', 'gender:confidence', 'description', 'fav_number','link_color',
       'retweet_count', 'sidebar_color', 'text', 'tweet_count', 'tweet_id', 'tweet_location', 'user_timezone'
       ]

df_preprocessed = df_cleaned[col]

# Remove rows where gender is 'Unknown'
df_preprocessed = df_preprocessed[df_preprocessed['gender'] != 'unknown']

# Scaling numerical features
scaler = StandardScaler()
df_preprocessed[['tweet_count', 'retweet_count', 'fav_number', 'gender:confidence']] = scaler.fit_transform(df_preprocessed[['tweet_count', 'retweet_count', 'fav_number', 'gender:confidence']])

# List of categorical columns
categorical_columns = ['link_color', 'sidebar_color', 'tweet_location', 'user_timezone']

# Initialize OneHotEncoder
one_hot_encoder = OneHotEncoder(drop='first', sparse=False)  # drop='first' prevents the dummy variable trap (removes first category)

# Apply OneHotEncoder using ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', one_hot_encoder, categorical_columns)
    ],
    remainder='passthrough'  # Keeps the rest of the columns as they are (numerical features)
)

# Fit and transform the data
df_preprocessed = preprocessor.fit_transform(df_preprocessed)

# Convert the transformed array back to a DataFrame and retain column names
encoded_feature_names = one_hot_encoder.get_feature_names_out(categorical_columns)
df_preprocessed = pd.DataFrame(df_preprocessed, columns=encoded_feature_names)

# change male=>0, female=>1, brand=>2
df_preprocessed.loc[df['gender'] == 'male', 'gender'] = 0
df_preprocessed.loc[df['gender'] == 'female', 'gender'] = 0
df_preprocessed.loc[df['gender'] == 'brand', 'gender'] = 1

# Check the first few rows of the preprocessed data
df_preprocessed.head()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

# drop columns that are irrelevant

col = ['gender', 'gender:confidence', 'description', 'fav_number','link_color',
       'retweet_count', 'sidebar_color', 'text', 'tweet_count','tweet_id'
       ]

df_preprocessed = df_cleaned[col]

# Remove rows where gender is 'Unknown'
df_preprocessed = df_preprocessed[df_preprocessed['gender'] != 'unknown']

# Scaling numerical features
scaler = StandardScaler()
df_preprocessed[['tweet_count', 'retweet_count', 'fav_number']] = scaler.fit_transform(df_preprocessed[['tweet_count', 'retweet_count', 'fav_number']])

# change male=>0, female=>1, brand=>2
df_preprocessed.loc[df['gender'] == 'male', 'gender'] = 0
df_preprocessed.loc[df['gender'] == 'female', 'gender'] = 0
df_preprocessed.loc[df['gender'] == 'brand', 'gender'] = 1

# Check the first few rows of the preprocessed data
df_preprocessed.head()

# Distribution of gender
plt.figure(figsize=(8, 6))
sns.countplot(x='gender', data=df_preprocessed)
plt.title('Distribution of Gender')
plt.xlabel('Gender')
plt.ylabel('Count')
plt.show()

# The imbalanceness can be handled either using the model attribute class_weight or applying sampling techniques.

"""NLP Processing"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

df_status = df_preprocessed.copy()
df_status = pd.concat([df_status['gender'], df_status['description']], axis=1)

df_status

# make all lowercase since "Run" is not the same as "run" for machine computation
import re

description = []

for x in df_status['description']:
    desc = re.sub("[^a-zA-Z]"," ",x)
    desc = desc.lower()
    description.append(desc)

df_status['description'] = description
df_status

# remove stopwords in sentence ==> i,a,the,an,and,.,me,........
def remove_stopwords(text):
    words = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return filtered_words


df_status['tokenized'] = df_status['description'].apply(remove_stopwords)
df_status

# count word in sentence by changing tokenized to vectorizor (for machine compute)
# CountVectorizer input must be string with one long list

from sklearn.feature_extraction.text import CountVectorizer

max_features = 1500
corpus = [' '.join(words) for words in df_status['tokenized']]

vectorizer = CountVectorizer(max_features = max_features, stop_words = "english")
X = vectorizer.fit_transform(corpus).toarray()

# let's see X in dataframe
df_ = pd.DataFrame(X, columns=vectorizer.get_feature_names_out(), index=df_status.index)

df_

y = df_preprocessed['gender'].values # Create an array
y # gender ==> our target in the model

# Now drop the processed columns ('description', 'text', and categorical) from the original dataset
df_preprocessed = df_preprocessed.drop(columns=['description', 'text', 'link_color', 'sidebar_color'])

#WITH TARGET INFORMATION REMOVED
df_preprocessed_X = df_preprocessed.drop(columns=['gender', 'gender:confidence'])

# Combine the text features with the other preprocessed features
X_combined = np.hstack((df_preprocessed_X.values, X))

df_preprocessed.head()

"""## Regression Tasks
The choosen regression model will be a gradient boosted regression decision tree.
"""

y = df_preprocessed["gender:confidence"]

from sklearn.ensemble import GradientBoostingRegressor

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

boosted_reg = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=3, random_state=42)

# Fit the model
boosted_reg.fit(X_train, y_train)

from sklearn.metrics import mean_squared_error

# Make predictions
y_pred = boosted_reg.predict(X_test)

# Evaluate performance using Mean Squared Error
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

y_tot_pred = boosted_reg.predict(X_combined)
mse = mean_squared_error(y, y_tot_pred)
print(f"Mean Squared Error: {mse}")

boosted_reg.feature_importances_

"""Find the rows with the largest difference in gender confidence"""



"""# Example Usage"""

import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# Convert data into DMatrix format, which is the format that XGBoost expects
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define XGBoost parameters
params = {
    'objective': 'multi:softmax',  # Specify the objective for multi-class classification
    'num_class': len(np.unique(y_train)),  # Number of classes
    'max_depth': 3,  # Maximum tree depth
    'eta': 0.1,  # Learning rate
    'subsample': 0.8,  # Fraction of samples used for training each tree
    'colsample_bytree': 0.8,  # Fraction of features used for training each tree
    'eval_metric': 'mlogloss'  # Evaluation metric
}

# Train the XGBoost model
num_round = 100  # Number of boosting rounds
bst = xgb.train(params, dtrain, num_round)

# Make predictions on the test set
y_pred = bst.predict(dtest)

# Calculate accuracy
accuracy = accuracy_score(y_test.tolist(), y_pred.tolist())

print(f"Accuracy: {accuracy:.2f}")

# Generate the classification report
report = classification_report(y_test.tolist(), y_pred.tolist())

# Print the classification report
print("XGBoost Classification Report:\n", report)

from sklearn.naive_bayes import GaussianNB

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)

# change y_train, y_test dtype from object to int64
y_train = y_train.astype(np.int64)
y_test = y_test.astype(np.int64)

# Create a Naive Bayes classifier (Gaussian Naive Bayes)
nb_classifier = GaussianNB()

# Fit the classifier to the training data
nb_classifier.fit(X_train, y_train)

# Make predictions on the test data
y_pred = nb_classifier.predict(X_test)

# Generate the classification report
report = classification_report(y_test, y_pred)

# Print the classification report
print("Naive Bayes Classification Report:\n", report)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score
from mlxtend.frequent_patterns import apriori, association_rules

# Load the dataset
data = pd.read_csv('twitter_user_data.csv', encoding='ISO-8859-1')  # Replace with actual file path

# Step 1: Data Cleaning
# Dropping columns with too many missing values or irrelevant ones
data_cleaned = data.drop(['_unit_id', '_last_judgment_at', 'profileimage', 'tweet_id', 'tweet_created'], axis=1)

# Fill missing values (example: fill numerical columns with mean, categorical with mode)
data_cleaned['fav_number'].fillna(data_cleaned['fav_number'].mean(), inplace=True)
data_cleaned['gender'].fillna(data_cleaned['gender'].mode()[0], inplace=True)
data_cleaned['tweet_location'].fillna('Unknown', inplace=True)

# Step 2: Handle Categorical Variables
# Convert categorical columns to numeric using OneHotEncoding or get_dummies
categorical_columns = ['_unit_state', 'gender', 'profile_yn', 'tweet_location']  # Add other categorical columns if necessary
data_cleaned = pd.get_dummies(data_cleaned, columns=categorical_columns, drop_first=True)

# Step 3: Define Target and Features
# Target: 'fav_number' (example), Features: all other columns
X = data_cleaned.drop('fav_number', axis=1)
y = data_cleaned['fav_number']

# Step 4: Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Step 5: Scale the Data (Now that all features are numeric)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Fit Ridge Regression Model
ridge_reg = Ridge(alpha=1.0)  # Alpha is the regularization strength
ridge_reg.fit(X_train_scaled, y_train)

# Step 7: Predict and Evaluate
y_pred = ridge_reg.predict(X_test_scaled)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse}")
print(f"R^2 Score: {r2}")

# Step 8: Apriori Algorithm (Assuming we are interested in user behavior patterns)
# For apriori, we need to transform data into boolean values (e.g., presence of an attribute)
transactions = data_cleaned[['retweet_count', 'profile_yn_True', 'tweet_location_Unknown']] > 0

# Apply the apriori algorithm
frequent_itemsets = apriori(transactions, min_support=0.05, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

print(frequent_itemsets)
print(rules)
